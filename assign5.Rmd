Assignment 5 Hadoop
-------------------

**Grading**: Work questions 1 and 2 for the 30 points for the assignment. You can get up to 20 points extra for questions 3, 4, and 5. If you stop prior to question 5, then show partial output for each queston.

The data comes from Internet Information Server (IIS) logs for msnbc.com and news-related portions of msn.com for the entire day of September, 28, 1999 (Pacific Standard Time). Each sequence in the dataset corresponds to page views of a user during that twenty-four hour period. Each event in the sequence corresponds to a user's request for a page. Requests are not recorded at the finest level of detail---that is, at the level of URL, but rather, they are recorded at the level of page category (as determined by a site administrator). The categories are "frontpage", "news", "tech", "local", "opinion", "on-air", "misc", "weather", "health", "living", "business", "sports", "summary", "bbs" (bulletin board service), "travel", "msn- news", and "msn-sports". Any page requests served via a caching mechanism were not recorded in the server logs and, hence, not present in the data.

Create two R scripts `mapper.R` and `reducer.R`plus pre- and post-processing R code. The objective is to explore how the relationships among the page categories change over the course of the day. There are 17 page categories.

1. Preprocess (and subset) the data by pushing a 1 into the first position of the first 1,000 observations out of the first 100,000 users, a 2 into the  first position out of the first 1,000 observations from the second 100,000 users, etc. for all 989,818 users. This results in 10 batches of 1,000 each with the batch number in the first position. These 10 values (1 to 10) will become the keys the the `mapper.R` code in the next item and represent a time index throughout ethe day.

```{r}
# Read in the data
msnbc.lines <- readLines("msnbc.txt")

# msnbc.lines is a very long vector
length(msnbc.lines)
head(msnbc.lines)

# Loop through the data, compute keys, write relevant rows to output file
outfile <- file("msnbc_preprocessed.txt", "w")

# Put your R code here

counter <- 0
first1000 <- 0
value <- 1
counter2 <- 0


while( TRUE ){
  
  
  counter = counter + 1
  counter2 = counter2 + 1
  currentLine <-  msnbc.lines[counter2]
  if( counter2 >= length(msnbc.lines) ){
    break
  }
  
  
  currentFields <- currentLine #strsplit( currentLine , "," ) )
  if(counter <=1000)
  {
    result <- paste( value , currentFields, sep="::" )
  
  
  cat(result, "\n", file=outfile, append=TRUE)
  cat(result, "\n", file="output.csv", append=TRUE)
  }
  if(counter == 100000)
  {
    counter = 0
    value = value + 1
  }
}
# close the connection to the output file.
close(outfile)
```

2. Develop the code for `mapper.R`. The key should be 1 for the first 1,000 observations (batch 1), 2 for the second 1,000 observatons (batch 2), etc. The value consists of the counts for each of the 17 page categories for each user-session. Note that the time order for each user-session is lost.

3. Develop the code for `reducer.R`. Create a `data.frame` for each batch. For each `data.frame`,  change all counts > 1 to 1. Compute the Jaccard distance matrix among the page categories. Note the Jaccard distance is $1 - J$ where $J$ is the Jaccard similarity. Vectorize the result as the return value (the length of the resulting vector will be $17 \times 16/2$). The key is the batch number.     
Hint: Tranpose the `0-1` matrix and multiply it by itself to get a $17 \times 17$ incidence matrix. Compute the Jaccard distanes from this matrix. Then do something like `yourMatrix[lower.tri(yourMatrix)]` to convert to a vector.

4. Outside Hadoop, post-process the output from the reducer. For each batch reconstruct the Jaccard distance matrix and convert to a `dist` object, e.g., using `as.dist`. Cluster each of the 10 batches in order using the `average` method in `hclust`. Discuss the changes in the dendrograms over time.

5. Develop the workflow for items 1 to 4 above with items 2 and 3 being run within HDFS/ Hadoop using Hadoop streaming based on the R scripts. If you cannot get the code to run with Hadoop then use UNIX pipes. Alternately, and perhaps peferably, develop the workflow using RHadoop entirely within the Rmd document.


